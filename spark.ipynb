{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Spark example\n",
    "\n",
    "Almond comes with a Spark integration module based on [ammonite-spark](https://github.com/alexarchambault/ammonite-spark).\n",
    "To use it, we have to import the *almond-spark* dependency as well as Spark 2.x itself.\n",
    "\n",
    "*ammonite-spark* handles loading Spark in a clever way, and does not rely on a specific Spark distribution.\n",
    "Because of that, you can load the Spark 2.x version of your choice. The only limitation is that the Scala version of Spark and the running Almond kernel must match.\n",
    "For Scala 2.12, at least Spark 2.4.0 is required.\n",
    "\n",
    "For more information, see the [README](https://github.com/alexarchambault/ammonite-spark/blob/master/README.md) of ammonite-spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`sh.almond::almond-spark:0.2.3-SNAPSHOT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// silence logging\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a `SparkSession` using the builder provided by *almond-spark*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can also connect to a real cluster. *ammonite-spark* currently supports standalone and *yarn* clusters. See its [README](https://github.com/alexarchambault/ammonite-spark/blob/master/README.md) for details.\n",
    "\n",
    "Now we can get a `SparkContext` from our `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then create an `RDD` and run some calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rdd = sc.parallelize(1 to 100000000, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now you should see a progress bar, showing the progress of the running Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val n = rdd.map(_ + 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val n = rdd.map(n => (n % 10, n)).reduceByKey(_ + _).collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (snapshot)",
   "language": "scala",
   "name": "scala-snapshot"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
